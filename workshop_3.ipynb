{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "workshop-3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyV6K_FGEXn1"
      },
      "source": [
        "# <center> Implementation of Neural Style Transfer</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UclkPnYTEvSF"
      },
      "source": [
        "## Overview\r\n",
        "- NST: Training an Image\r\n",
        "- Style and Content Images\r\n",
        "- Feature extraction from style and content images\r\n",
        "- Gram Matrix\r\n",
        "- Loss Function for NST\r\n",
        "  - Style Loss\r\n",
        "  - Content Loss\r\n",
        "- Generate Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogNTs3p-OxiL"
      },
      "source": [
        "#### We will first import all necessary libraries and frameworks we are going to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NoehRrVO2to"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "import cv2\r\n",
        "\r\n",
        "from tensorflow.keras.applications import VGG19\r\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input\r\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA41NA1bFmyS"
      },
      "source": [
        "### NST: Training an Image\r\n",
        "<br>\r\n",
        "How do we generate an Image in Neural Style Transfer?\r\n",
        "\r\n",
        "In normal problems(i.e. Classification, Regression etc), we make a model(set of operations and operands(weights,biases and data) and train the model(weights and biases) to fit the distribution of the data), but in Neural Style Transfer, we take pixel values of the image as our weights and biases and train the image(i.e. change pixel values during training) and generate the Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWuJuGdCHOQb"
      },
      "source": [
        "### Style and Content Images\r\n",
        "<br><br>\r\n",
        "#### Style Image:\r\n",
        "Image from which we want to take the most basic information, i.e. edges, simple and fundamental shapes, basic color schemes, textures, etc.\r\n",
        "!['Style Image'](https://miro.medium.com/max/767/1*B5zSHvNBUP6gaoOtaIy4wg.jpeg)\r\n",
        "<br><br>\r\n",
        "#### Content Image:\r\n",
        "Image from which we want to take complex shapes, i.e. combination or mixture of shapes and colors. <br>For example: a dog is in the content image, we want a dog in our generated image.\r\n",
        "!['Content Image'](https://i.ytimg.com/vi/xVJwwWQlQ1o/maxresdefault.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvuGmZ1HI7q_"
      },
      "source": [
        "#### Loading an image and converting the image into a Tensor\r\n",
        "<br>\r\n",
        "Images comes in various sizes, some are small in size and some are very large, to perform NST on very large images is very time-consuming. Hence we will use image of maximum height/width of 512, if image has height/width of more than 512, we will rescale the image with max height/width of 512 while maintaining the ratio of height/width."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rlMUAeOBwGg"
      },
      "source": [
        "def load_image(image_path,max_length):\r\n",
        "    \r\n",
        "    if os.path.exists(image_path):\r\n",
        "        image = cv2.imread(image_path)\r\n",
        "        \r\n",
        "        height,width = image.shape[0:2]\r\n",
        "        \r\n",
        "        if (height>width):\r\n",
        "            if (height>max_length):\r\n",
        "                width = int(width*(max_length/height))\r\n",
        "                height = max_length\r\n",
        "                image = cv2.resize(image,(width,height),interpolation=cv2.INTER_AREA)\r\n",
        "        else:\r\n",
        "            if (width>max_length):\r\n",
        "                height = int(height*(max_length/width))\r\n",
        "                width = max_length\r\n",
        "                image = cv2.resize(image,(width,height),interpolation=cv2.INTER_AREA)\r\n",
        "        \r\n",
        "        image_rgb = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\r\n",
        "        image_rgb = np.expand_dims(image_rgb,axis=0)\r\n",
        "        image_tensor = tf.convert_to_tensor(image_rgb)\r\n",
        "        image_tensor = tf.cast(image_tensor,tf.float32)\r\n",
        "        image_tensor = image_tensor/255.\r\n",
        "\r\n",
        "        return image_tensor\r\n",
        "    else:\r\n",
        "        raise OSError('Invalid Image path!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7g5lXnOmLlaj",
        "outputId": "12e840cd-b4cd-4bf6-a485-b4324ecd6a32"
      },
      "source": [
        "image = load_image('drive/MyDrive/nst_workshop/neural_style_transfer/content/waterfall.jpeg',512)\r\n",
        "print(image.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 512, 341, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGzHvjCbNmhD"
      },
      "source": [
        "### Feature Extraction from style and content images\r\n",
        "\r\n",
        "We need to extract features from style and content images so that we can compare features from generated image with style and content images and bring the generated image more closer to content and style images in specific ways.\r\n",
        "<br><br>\r\n",
        "To do feature extraction, we need to define a feature extractor, we will use VGG19 architecture of CNN(pre-trained) for this purporse. \r\n",
        "!['VGG19'](https://www.researchgate.net/profile/Clifford_Yang/publication/325137356/figure/fig2/AS:670371271413777@1536840374533/llustration-of-the-network-architecture-of-VGG-19-model-conv-means-convolution-FC-means.jpg)\r\n",
        "\r\n",
        "We do not need the whole VGG16, we just need specific layers from the VGG19:\r\n",
        "- Initial layers for Style(for basic info)\r\n",
        "- Mid layers for Content(for more complex info)\r\n",
        "\r\n",
        "Hence, for style and content, we will use following layers of VGG19:\r\n",
        "- Style: [block1_conv1, block2_conv1, block3_conv1, block4_conv1,block5_conv1]\r\n",
        "- Content: [block4_conv2]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSeWLYUXOtdQ",
        "outputId": "89a0fa17-adc7-49e5-8350-720be268f5cf"
      },
      "source": [
        "def create_feature_extractor(convnet,style_layers,content_layers):\r\n",
        "        layers = style_layers+content_layers\r\n",
        "        outputs = [convnet.get_layer(layer_name).output for layer_name in layers]\r\n",
        "        model = tf.keras.models.Model(convnet.inputs,outputs)\r\n",
        "        \r\n",
        "        return model\r\n",
        "\r\n",
        "\r\n",
        "content_layers = ['block4_conv2']\r\n",
        "style_layers = ['block1_conv1','block2_conv1','block3_conv1','block4_conv1','block5_conv1']\r\n",
        "\r\n",
        "convnet = VGG19(include_top=False,weights='imagenet')\r\n",
        "feature_extractor = create_feature_extractor(convnet,style_layers,content_layers)\r\n",
        "feature_extractor.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80142336/80134624 [==============================] - 0s 0us/step\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, None, None, 3)]   0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "block3_conv4 (Conv2D)        (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv4 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "=================================================================\n",
            "Total params: 12,944,960\n",
            "Trainable params: 12,944,960\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUQGXdA0QdYb"
      },
      "source": [
        "### Gram Matrix\r\n",
        "\r\n",
        "We need to extract basic info from style image and compare it with basics of generated image in a specific way, that specific way is 'Gram Matrix'.\r\n",
        "\r\n",
        "We have output(feature maps) from style layers that we defined. we flatten each feature map from a layer into a vector and stack it and make a matrix for each layer. Now we multiply this matrix with its transpose and the resultant matrix is called 'Gram Matrix'.\r\n",
        "<br><br>\r\n",
        "Flatten<br>\r\n",
        "!['Flatten'](https://www.w3resource.com/w3r_images/numpy-manipulation-ndarray-flatten-function-image-1.png)\r\n",
        "<br><br>\r\n",
        "\r\n",
        "Intuition behind 'Gram Matrix':\r\n",
        "<br>\r\n",
        "We need a way to compare the relation of shapes, edges, textures between style image and generated image. In style image we saw(starry night), there are individual strokes form circular shapes, each edge of a individual stroke in way has a relation with nearby circular shape, and that is what we call style in which the image was painted.\r\n",
        "!['Style Image'](https://miro.medium.com/max/767/1*B5zSHvNBUP6gaoOtaIy4wg.jpeg)\r\n",
        "Now we want that relation in our generated image as well, hence we need the relation between edges of individual strokes and the circular shapes, we assume that edges of individual strokes and circular shapes would be detected individually by atleast one of filters of the 'style layers'. So we extract relation between output of all filters in a layer, in all layers.\r\n",
        "!['Feature maps'](https://adeshpande3.github.io/assets/deconvnet.png)\r\n",
        "<br><br>\r\n",
        "How do we extract relation between output of all filters?\r\n",
        "\r\n",
        "we use dot-product between vectors. Dot-product between two vectors gives us information on how similar those two vectors are.<br>\r\n",
        "!['dot product'](https://ml-cheatsheet.readthedocs.io/en/latest/_images/khan_academy_matrix_product.png)<br>\r\n",
        "So we flatten each output from the filters of a layer, stack them in a matrix, and do matrix multiplication of the matrix with its transpose so that all the combinations of dot-product between two vectors from the matrix takes place.\r\n",
        "<br><br>\r\n",
        "After calculating relations between each feature maps, we compare the relations in generated image and style image and bring them closer to each other by training the generated image. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLktZLX2QQA_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84d8d797-418a-4626-c074-f0a54f751a20"
      },
      "source": [
        "def gram_matrix(tensor):\r\n",
        "        shape = tensor.get_shape()\r\n",
        "        num_channels = int(shape[3])\r\n",
        "        matrix = tf.reshape(tensor, shape=[-1, num_channels])\r\n",
        "        gram = tf.expand_dims(tf.matmul(tf.transpose(matrix), matrix),axis=0)\r\n",
        "\r\n",
        "        return gram\r\n",
        "\r\n",
        "\r\n",
        "temp_input_tensor = tf.constant(np.random.randn(1,5,5,512))\r\n",
        "gram_matrix_tensor = gram_matrix(temp_input_tensor)\r\n",
        "print(gram_matrix_tensor.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 512, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgORlTRoVwRT"
      },
      "source": [
        "### Loss function for NST\r\n",
        "\r\n",
        "Content Loss:<br>\r\n",
        "Content Loss is calculated by difference between feature maps of content layer filters. Feature maps of content layer for generated image and content image are compared directly because we want spatial information from content image.\r\n",
        "\r\n",
        "Style Loss:<br>\r\n",
        "Style Loss is calculated by difference between gram matrix of all feature maps of style layers. Gram Matrix of all feature maps of style layers for generated image and content image are compared directly, so that we can get same texture in the generated image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3w51oZ6lVpBG"
      },
      "source": [
        "def loss(outputs,style_features,content_features,style_weight,content_weight):\r\n",
        "        style_outputs = outputs[0]\r\n",
        "        content_outputs = outputs[1]\r\n",
        "        \r\n",
        "        style_loss = []\r\n",
        "        for index,style_output in enumerate(style_outputs):\r\n",
        "            loss = tf.reduce_mean((style_output - style_features[index])**2)\r\n",
        "            style_loss.append(loss)\r\n",
        "        \r\n",
        "        style_loss = tf.add_n(style_loss)\r\n",
        "        style_loss *= style_weight\r\n",
        "        \r\n",
        "        \r\n",
        "        content_loss = []\r\n",
        "        for index,content_output in enumerate(content_outputs):\r\n",
        "            loss = tf.reduce_mean((content_output - content_features[index])**2)\r\n",
        "            content_loss.append(loss)\r\n",
        "        \r\n",
        "        content_loss = tf.add_n(content_loss)\r\n",
        "        content_loss *= content_weight\r\n",
        "        \r\n",
        "        loss = style_loss + content_loss\r\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4bp_Jo_Xae_"
      },
      "source": [
        "### Generate Image\r\n",
        "\r\n",
        "We have Loss function, images, feature extractor. Let's put it all together and make a NST machine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15WkXLEFXW3c"
      },
      "source": [
        "class style_transfer(tf.keras.models.Model):\r\n",
        "    \r\n",
        "    def __init__(self,convnet,input_preprocessor,style_layers,content_layers,style_weight,content_weight):\r\n",
        "        \r\n",
        "        super(style_transfer, self).__init__()\r\n",
        "        \r\n",
        "        self.feature_extractor = self.create_feature_extractor(convnet,style_layers,content_layers)\r\n",
        "        self.feature_extractor.trainable = False\r\n",
        "        self.input_preprocessor_ = input_preprocessor\r\n",
        "        self.style_layers = style_layers\r\n",
        "        self.content_layers = content_layers\r\n",
        "        self.num_style_layers = len(style_layers)\r\n",
        "        self.style_weight = style_weight/self.num_style_layers\r\n",
        "        self.content_weight = content_weight/len(content_layers)\r\n",
        "        \r\n",
        "\r\n",
        "    def call(self,inputs):\r\n",
        "        \r\n",
        "        inputs = inputs*255\r\n",
        "        preprocessed_inputs = self.input_preprocessor_(inputs)\r\n",
        "        \r\n",
        "        outputs = self.feature_extractor(preprocessed_inputs)\r\n",
        "        style_outputs = outputs[:self.num_style_layers]\r\n",
        "        content_outputs = outputs[self.num_style_layers:]\r\n",
        "        \r\n",
        "        style_outputs = [self.gram_matrix(style_output) for style_output in style_outputs]\r\n",
        "        \r\n",
        "        return [style_outputs,content_outputs]\r\n",
        "    \r\n",
        "    \r\n",
        "    @staticmethod\r\n",
        "    def create_feature_extractor(convnet,style_layers,content_layers):\r\n",
        "        layers = style_layers+content_layers\r\n",
        "        outputs = [convnet.get_layer(layer_name).output for layer_name in layers]\r\n",
        "        model = tf.keras.models.Model(convnet.inputs,outputs)\r\n",
        "        \r\n",
        "        return model\r\n",
        "    \r\n",
        "    @staticmethod\r\n",
        "    def gram_matrix(tensor):\r\n",
        "        shape = tensor.get_shape()\r\n",
        "        num_channels = int(shape[3])\r\n",
        "        matrix = tf.reshape(tensor, shape=[-1, num_channels])\r\n",
        "        gram = tf.expand_dims(tf.matmul(tf.transpose(matrix), matrix),axis=0)\r\n",
        "\r\n",
        "        return gram\r\n",
        "    \r\n",
        "    \r\n",
        "    @staticmethod\r\n",
        "    def loss(outputs,style_features,content_features,style_weight,content_weight):\r\n",
        "        style_outputs = outputs[0]\r\n",
        "        content_outputs = outputs[1]\r\n",
        "        \r\n",
        "        style_loss = []\r\n",
        "        for index,style_output in enumerate(style_outputs):\r\n",
        "            loss = tf.reduce_mean((style_output - style_features[index])**2)\r\n",
        "            style_loss.append(loss)\r\n",
        "        \r\n",
        "        style_loss = tf.add_n(style_loss)\r\n",
        "        style_loss *= style_weight\r\n",
        "        \r\n",
        "        \r\n",
        "        content_loss = []\r\n",
        "        for index,content_output in enumerate(content_outputs):\r\n",
        "            loss = tf.reduce_mean((content_output - content_features[index])**2)\r\n",
        "            content_loss.append(loss)\r\n",
        "        \r\n",
        "        content_loss = tf.add_n(content_loss)\r\n",
        "        content_loss *= content_weight\r\n",
        "        \r\n",
        "        loss = style_loss + content_loss\r\n",
        "        return loss\r\n",
        "    \r\n",
        "    \r\n",
        "    @staticmethod\r\n",
        "    def clip_pixels(image):\r\n",
        "        clipped_image = tf.clip_by_value(image,clip_value_min = 0.0,clip_value_max = 1.0)\r\n",
        "        return clipped_image  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXEVXRoRXxDU"
      },
      "source": [
        "A function to convert TensorFlow tensor to Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvtZWxAiXtjS"
      },
      "source": [
        "def tensor_to_image(tensor):\r\n",
        "\t\ttensor = tensor * 255\r\n",
        "\t\ttensor = np.array(tensor, dtype=np.uint8)\r\n",
        "\r\n",
        "\t\tif np.ndim(tensor) > 3:\r\n",
        "\t\t\ttensor = tensor[0]\r\n",
        "\r\n",
        "\t\treturn Image.fromarray(tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBelXs8CZ243"
      },
      "source": [
        "Defining content-layers and style-layers.\r\n",
        "\r\n",
        "We also define style_weight, content_weight: the amount by which we want style to be added and content to be added in generated image.\r\n",
        "\r\n",
        "There is also one more weight here, tv_weight(total_variational_weight). This weight is applied to total_variational_loss. This loss measures the amount of noise in an image, and reducing this loss reduces the noise and gives a smooth image.\r\n",
        "\r\n",
        "We define #epochs and #steps_per_epoch(number of times we will change the pixel values of the generated image per epoch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbTojpU0X2FB"
      },
      "source": [
        "content_layers = ['block4_conv2']\r\n",
        "style_layers = ['block1_conv1','block2_conv1','block3_conv1','block4_conv1','block5_conv1']\r\n",
        "\r\n",
        "style_weight = 10.0\r\n",
        "content_weight = 1e7\r\n",
        "tv_weight = 20.0\r\n",
        "\r\n",
        "epochs = 15\r\n",
        "steps_per_epoch = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKLxtmFmawyn"
      },
      "source": [
        "We make folders/directories for style,content,generated image in our drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2PIw7LRX5PU"
      },
      "source": [
        "if not os.path.exists('drive/MyDrive/neural_style_transfer'):\r\n",
        "  os.mkdir('drive/MyDrive/neural_style_transfer')\r\n",
        "\r\n",
        "if not os.path.exists('drive/MyDrive/neural_style_transfer/content'):\r\n",
        "  os.mkdir('drive/MyDrive/neural_style_transfer/content')\r\n",
        "\r\n",
        "if not os.path.exists('drive/MyDrive/neural_style_transfer/style'):\r\n",
        "  os.mkdir('drive/MyDrive/neural_style_transfer/style')\r\n",
        "\r\n",
        "if not os.path.exists('drive/MyDrive/neural_style_transfer/outputs'):\r\n",
        "  os.mkdir('drive/MyDrive/neural_style_transfer/outputs')\r\n",
        "\r\n",
        "if not os.path.exists('drive/MyDrive/neural_style_transfer/outputs/intermediate'):\r\n",
        "  os.mkdir('drive/MyDrive/neural_style_transfer/outputs/intermediate')\r\n",
        "\r\n",
        "if not os.path.exists('drive/MyDrive/neural_style_transfer/outputs/final'):\r\n",
        "  os.mkdir('drive/MyDrive/neural_style_transfer/outputs/final')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-4AvtLXa6PN"
      },
      "source": [
        "Define the path to content, style and generated image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha61IMoWX8cC"
      },
      "source": [
        "content_image_path = 'drive/MyDrive/neural_style_transfer/content/Einstein.jpg' #Einstein.jpg\r\n",
        "style_image_path = 'drive/MyDrive/neural_style_transfer/style/la_muse.jpg' #la_muse.jpg\r\n",
        "\r\n",
        "final_output_path = 'drive/MyDrive/neural_style_transfer/outputs/final/'\r\n",
        "intermediate_output_path = 'drive/MyDrive/neural_style_transfer/outputs/intermediate/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuquN04fa93j"
      },
      "source": [
        "Loading content and style image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQXFVY3vYAOD"
      },
      "source": [
        "content_image = load_image(content_image_path,512)\r\n",
        "style_image = load_image(style_image_path,512)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5mEW6AkZVet",
        "outputId": "624b60ac-1e4e-4994-fb47-999cbd758778"
      },
      "source": [
        "print('Content_image size:',content_image.shape)\r\n",
        "print('Style_image size:',style_image.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Content_image size: (1, 389, 320, 3)\n",
            "Style_image size: (1, 512, 512, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXM_x8l-bDNA"
      },
      "source": [
        "We load predefined convnet for feature extraction and make an instance of NST machine.\r\n",
        "\r\n",
        "We also load the optimizer for training the generated image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvtDLEchYDmM"
      },
      "source": [
        "convnet = VGG19(include_top=False,weights='imagenet')\r\n",
        "\r\n",
        "nst = style_transfer(convnet,preprocess_input,style_layers,content_layers,style_weight,content_weight)\r\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01,beta_1=0.99,epsilon=1e-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpLkrF0GbPs0"
      },
      "source": [
        "We calculate the feature maps of content-layer for content image and Gram-Matrix of feature maps of style-layers for style image "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYV57aZqYJ0k"
      },
      "source": [
        "style_features = nst(style_image)[0]\r\n",
        "content_features = nst(content_image)[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnymTfmJbdYi"
      },
      "source": [
        "We make a copy of content image as generated image and define it as a Tensorflow variable so that we can train it.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjqJhJxwZeZp"
      },
      "source": [
        "generated_image = tf.Variable(content_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0pX9ci3boYp"
      },
      "source": [
        "we define path to final and intermediate outputs for generated-image based on name of content and style images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_POsxaNZjig"
      },
      "source": [
        "style_image_name = style_image_path.split('/')[-1].split('.')[0]\r\n",
        "content_image_name = content_image_path.split('/')[-1].split('.')[0]\r\n",
        "\r\n",
        "if not os.path.exists(intermediate_output_path+content_image_name+'_'+style_image_name):\r\n",
        "  os.mkdir(intermediate_output_path+content_image_name+'_'+style_image_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGpFMABFb1Hm"
      },
      "source": [
        "we define a function 'train_step'. In this function, we calcuated the feature maps of content-layer and Gram-Matrix of feature maps of style-layers for generated image, calculate the style-loss and content-loss, calculate gradients according to the loss and apply it to the generated image.\r\n",
        "\r\n",
        "And we call this function for #epochs x #steps_per_epoch times and we have the final image of NST!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_UEUeRmZj_v",
        "outputId": "9bea58e6-70fc-432e-c55a-06528b1f1d68"
      },
      "source": [
        "style_weight = 10.0\r\n",
        "content_weight = 1e4\r\n",
        "\r\n",
        "\r\n",
        "@tf.function\r\n",
        "def train_step(image,nst,optimizer,style_features,content_features,tv_weight):\r\n",
        "    \r\n",
        "    with tf.GradientTape() as tape:\r\n",
        "        outputs = nst(image)\r\n",
        "        loss = nst.loss(outputs,style_features,content_features,nst.style_weight,nst.content_weight)\r\n",
        "        loss += tv_weight * tf.image.total_variation(image)\r\n",
        "    \r\n",
        "    gradients = tape.gradient(loss,image)\r\n",
        "    optimizer.apply_gradients([(gradients,image)])\r\n",
        "    image.assign(nst.clip_pixels(image))\r\n",
        "\r\n",
        "\r\n",
        "generated_image = tf.Variable(content_image)\r\n",
        "\r\n",
        "\r\n",
        "for epoch in range(epochs):\r\n",
        "  for step in range(steps_per_epoch):\r\n",
        "\r\n",
        "    train_step(generated_image,nst,optimizer,style_features,content_features,tv_weight)\r\n",
        "  print(f'\\nEpoch: {epoch+1}\\nTotal Steps: {(epoch+1)*steps_per_epoch}')\r\n",
        "  tensor_to_image(generated_image).save(intermediate_output_path+content_image_name+'_'+style_image_name+'/'+str(epoch+1)+'.jpg')\r\n",
        "\r\n",
        "print('\\nSaving Final Image:')\r\n",
        "tensor_to_image(generated_image).save(final_output_path+content_image_name+'_'+style_image_name+'.jpg')\r\n",
        "print('Final Image Saved')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 1\n",
            "Total Steps: 100\n",
            "\n",
            "Epoch: 2\n",
            "Total Steps: 200\n",
            "\n",
            "Epoch: 3\n",
            "Total Steps: 300\n",
            "\n",
            "Epoch: 4\n",
            "Total Steps: 400\n",
            "\n",
            "Epoch: 5\n",
            "Total Steps: 500\n",
            "\n",
            "Epoch: 6\n",
            "Total Steps: 600\n",
            "\n",
            "Epoch: 7\n",
            "Total Steps: 700\n",
            "\n",
            "Epoch: 8\n",
            "Total Steps: 800\n",
            "\n",
            "Epoch: 9\n",
            "Total Steps: 900\n",
            "\n",
            "Epoch: 10\n",
            "Total Steps: 1000\n",
            "\n",
            "Epoch: 11\n",
            "Total Steps: 1100\n",
            "\n",
            "Epoch: 12\n",
            "Total Steps: 1200\n",
            "\n",
            "Epoch: 13\n",
            "Total Steps: 1300\n",
            "\n",
            "Epoch: 14\n",
            "Total Steps: 1400\n",
            "\n",
            "Epoch: 15\n",
            "Total Steps: 1500\n",
            "\n",
            "Saving Final Image:\n",
            "Final Image Saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97PBuAUWZn3P"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}